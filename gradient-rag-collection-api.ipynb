{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and Installs"
      ],
      "metadata": {
        "id": "5zGObZ1xs6x2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q gradientai --upgrade"
      ],
      "metadata": {
        "id": "Dt5QJBVIncXn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59a1b44-28fa-4ee2-b920-84826fe9cd3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.5/375.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBkO7advwXjN"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import requests\n",
        "from pprint import pprint\n",
        "from google.colab import userdata\n",
        "from gradientai import Gradient"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GRADIENT_ACCESS_TOKEN'] = userdata.get('GRADIENT_ACCESS_TOKEN')\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = userdata.get('GRADIENT_WORKSPACE_ID')"
      ],
      "metadata": {
        "id": "PViEqUC-nZlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gradient = Gradient()"
      ],
      "metadata": {
        "id": "zwhlpwZBpFJr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creates a new collection of RAG documents"
      ],
      "metadata": {
        "id": "QMAIcok5szC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rag_collection = gradient.create_rag_collection(\n",
        "  name=\"Attention RAG 18th April\",\n",
        "  slug=\"bge-large\",\n",
        "  filepaths=[\n",
        "    \"attention.pdf\",\n",
        "  ],\n",
        ")"
      ],
      "metadata": {
        "id": "LFowYRCUnkt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(\"Rag Collection id is : {}\".format(rag_collection.id_))"
      ],
      "metadata": {
        "id": "BUa5MMH-oLLe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d0c8104-cce3-4ef8-e675-c004745c7a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Rag Collection id is : 1d5c82dd-0ce6-4c6f-82f9-f3f34d7c1dbd_rag_config'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mGqjDFvWogJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document question & answer"
      ],
      "metadata": {
        "id": "r6aXwNOSowhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = gradient.answer(\n",
        "    question=\"What is positional encoding?\",\n",
        "    source={\n",
        "        \"type\": \"rag\",\n",
        "        \"collectionId\" : rag_collection.id_\n",
        "    }\n",
        ")['answer']"
      ],
      "metadata": {
        "id": "koaKJTUzogMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pprint(result)"
      ],
      "metadata": {
        "id": "6pRoKSiLogPK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "893ecc96-6a1c-47f6-e8d1-c5030bd309f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('Based on the context information provided, positional encoding is a method '\n",
            " 'used in the Transformer model to inject information about the relative or '\n",
            " 'absolute position of the tokens in the sequence. Since the Transformer model '\n",
            " 'contains no recurrence or convolution, and therefore no inherent sense of '\n",
            " 'position or order, positional encodings are added to the input embeddings at '\n",
            " 'the bottoms of the encoder and decoder stacks. These positional encodings '\n",
            " 'have the same dimension as the embeddings, so that the two can be summed. In '\n",
            " 'the model described in the context, the positional encodings are implemented '\n",
            " 'as sine and cosine functions of different frequencies, with the wavelengths '\n",
            " 'forming a geometric progression. This allows the model to easily learn to '\n",
            " 'attend by relative positions and potentially extrapolate to sequence lengths '\n",
            " 'longer than those encountered during training.')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gBowiAn5ogR4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}